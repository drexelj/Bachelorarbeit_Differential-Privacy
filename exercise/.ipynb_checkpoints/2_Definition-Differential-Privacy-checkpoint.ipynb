{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6bd9dfd7",
   "metadata": {},
   "source": [
    "### Musterlösung zu den Kontrollfragen des vorherigen Notebooks\n",
    "\n",
    "* Welche Eigenschaft gibt die $k$-Anonymität für eine Datensammlung vor?\n",
    "> Eine Person (bzw. ein einzelner Datensatz) darf in einer Gruppe kleiner oder gleich $k$ nicht unterscheidbar sein. Es müssen somit stets mindestens $k$ identische Datensätze vorhanden sein.\n",
    "* Welches sind die Identifikatoren, Quasi-Identifikatoren und sensitiven Attribute in Tabelle 1?\n",
    "> Der Vor- und Nachname sind Identifikatoren. Das Geburtsdatum, Geschlecht und die PLZ sind Quasi-Identifikatoren. Die Krankheit ist ein sensitives Attribut.\n",
    "* Mit welcher Technik kann (unter anderem) die $k$-Anonymität geknackt werden?\n",
    "> Mit \"Verlinken\" bzw. dem Hinzuziehen von Hintergrundinformationen.\n",
    "\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d52fe1c",
   "metadata": {},
   "source": [
    "# Notebook 2: Definition von Differential Privacy\n",
    "\n",
    "***Hier geht es zum vorherigen Notebook dieser Serie: [Notebook 1: Einführung in die Thematik](./1_Einführung-in-die-Thematik.ipynb)***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caa7ce1e",
   "metadata": {},
   "source": [
    "## Lernziele\n",
    "\n",
    "Folgende Lernziele sollten mit der Bearbeitung dieses Jupyter Notebooks erreicht werden:\n",
    "- Die Teilnehmenden sind in der Lage, die beiden Modelle (lokal und zentral) der Differential Privacy zu unterscheiden.\n",
    "- Die Teilnehmenden sind in der Lage, die Definition der $\\varepsilon$-Differential Privacy zu erklären.\n",
    "- Die Teilnehmenden sind in der Lage, das Konzept des Privacy-Budgets zu erklären.\n",
    "- Die Teilnehmenden sind in der Lage, die Auswirkung des Privacy-Budgets auf den Wissensgewinn des Angreifers abzuschätzen.\n",
    "- Die Teilnehmenden sind in der Lage, das Konzept der Komposition des Privacy-Budgets zu bezeichnen."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c615b49b",
   "metadata": {},
   "source": [
    "## Wichtige Begriffe\n",
    "\n",
    "Die Differential Privacy ist eine Definition, welche Anforderungen an eine Funktion stellt. Entsprechend können die Eigenschaften der Differential Privacy nur für Funktionen erfüllt werden, nicht für eine Datensammlung. Dies ist ein essentieller Unterschied zu anderen Ansätzen, wie beispielsweise der $k$-Anonymität.\n",
    "\n",
    "Eine Funktion, welche die Eigenschaften der Differential Privacy erfüllt, wird als **Mechanismus** bezeichnet. Die Institution, welche den Mechanismus implementiert und damit die Daten sammelt oder publiziert, wird als **Aggregator** bezeichnet.\n",
    "\n",
    "Das Resultat eines Mechanismus ist vom Anwendungsfall abhängig und kann sich stark unterscheiden. Dies könnte die Ausgabe einer Datenbankabfrage, ein Histogramm, die Schnittstelle zu einem anderen Mechanismus u.v.m. sein. Das Resultat des Mechanismus wird in dieser Übung unabhängig von Anwendungsfall als **Ausgabe** bezeichnet. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a90b493",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "<b>Wichtige Erkenntnis</b>\n",
    "    <br />\n",
    "    <br /> \n",
    "Die Differential Privacy beschreibt die Eigenschaften einer Funktion (Mechanismus) und nicht die Eigenschaften einer Datensammlung (wie bspw. bei der k-Anonymität). Die Art des Mechanismus kann vielseitig sein. Zum Beispiel könnte dies eine einfache Datenbankabfrage wie \"COUNT()\" sein, aber auch eine statistische Anwendung zur Erstellung eines Histogramms basierend auf einer Datensammlung.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44aa2979",
   "metadata": {},
   "source": [
    "## Zwei Modelle der Differential Privacy\n",
    "\n",
    "Ein Mechanismus kann für das Sammeln der Daten, aber auch für das Publizieren der Daten implementiert werden. Deshalb wird allgemein zwischen zwei Modellen der Differential Privacy unterschieden."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca17e10a",
   "metadata": {},
   "source": [
    "### Das lokale Modell\n",
    "\n",
    "Beim lokalen Modell (<a href=\"#abb-1\">Abbildung 1</a>) wird der Mechanismus für das **Sammeln von Daten** implementiert. Hierbei wird der Mechanismus zwischen den Daten-Quellen (i.d.R. sind dies Personen, über welche Daten gesammelt wird) und dem Aggregator implementiert.\n",
    "<br>\n",
    "<br>\n",
    "<center>\n",
    "<img src=\"./src/Lokale-DP.png\" alt=\"Lokales Modell\">\n",
    "<br>\n",
    "<br>\n",
    "<a name=\"abb-1\">Abbildung 1: Das lokale Modell der Differential Privacy</center></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58d0e504",
   "metadata": {},
   "source": [
    "### Das zentrale Modell\n",
    "\n",
    "Beim zentralen Modell (<a href=\"#abb-2\">Abbildung 2</a>) wird der Mechanismus für die **Publikation von Daten** implementiert. Hierbei wird der Mechanismus zwischen dem Aggregator und der Ausgabe der Daten implementiert.\n",
    "<br>\n",
    "<br>\n",
    "<center>\n",
    "<img src=\"./src/Zentrale-DP.png\" alt=\"Zentrales Modell\">\n",
    "<br>\n",
    "<br>\n",
    "<a name=\"abb-2\">Abbildung 2: Das zentrale Modell der Differential Privacy</center></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4420081a",
   "metadata": {},
   "source": [
    "## Die Grundidee von Differential Privacy\n",
    "\n",
    "Die Grundidee von Differential Privacy liegt darin, dass ein Angreifer nicht herausfinden kann, ob eine bestimmte Person in einer Datensammlung enthalten ist oder nicht. Denn wenn ein Angreifer nicht bestimmen kann, ob eine Person enthalten ist oder nicht, kann der Angreifer auch nicht Informationen über diese Person erhalten. Dies soll auch erfüllt sein, wenn der Angreifer unbegrenzt Hintergrundinformationen hat. Das Maximum an Hintergrundinformationen ist dann erreicht, wenn ein Angreifer alle Datensätze kennt, ausser dem gesuchten Datensatz (= die gesuchte Person).\n",
    "\n",
    "Diese Eigenschaft wird nachfolgend anhand von zwei Datensammlungen abstrahiert (siehe auch <a href=\"#abb-3\">Abbildung 3</a>):\n",
    "\n",
    "* Es sei eine Datensammlung $D_1$ mit Anzahl $n$ Datensätzen gegeben.\n",
    "* Es existiert eine Kopie dieser Datensammlung, welche sich in genau einem Datensatz unterscheidet. Dies ist eine sogenannte **benachbarte Datensammlung** $D_2$ mit $n-1$ Datensätzen.\n",
    "* In $D_1$ ist die gesuchte Person enthalten und in $D_2$ ist diese nicht enthalten. Dies ist der einzige Unterschied zwischen diesen beiden Datensammlungen.\n",
    "* Ein Angreifer macht nun eine Abfrage und erhält die Ausgabe entweder von $D_1$ (gesuchte Person ist enthalten) oder $D_2$ (gesuchte Person ist nicht enthalten). Der Angreifer sieht nicht, von welcher der beiden Datensammlungen die Ausgabe stammte.\n",
    "* Die Differential Privacy verlangt nun, dass ein Angreifer bei der erhaltenen Ausgabe mit sehr hoher Wahrscheinlichkeit nicht unterscheiden kann, ob es sich bei der Quelle um $D_1$ oder $D_2$ handelte. Der Angreifer soll also basierend auf der erhaltenen Ausgabe nicht bestimmen können, ob die gesuchte Person enthalten war oder nicht.\n",
    "* Sind diese Eigenschaften für alle Datensätze gegeben, so kann ein Angreifer für keine der Personen herausfinden, ob sie enthalten sind oder nicht. Dadurch ist der Schutz jedes Individuums gewährleistet.\n",
    "<br>\n",
    "<br>\n",
    "<center>\n",
    "<img src=\"./src/Prinzip_DP.png\" alt=\"Grundidee Differential Privacy\">\n",
    "<br>\n",
    "<a name=\"abb-3\">Abbildung 3: Grundidee von Differential Privacy  </center></a>\n",
    "\n",
    "Für eine bessere Verständlichkeit wurde in diesem Beispiel in der benachbarten Datensammlung $D_2$ eine Zeile vollständig entfernt. Dies muss jedoch nicht zwingend der Fall sein. Es könnte auch ein Datensatz so verändert worden sein, dass die gesuchte Person nicht mehr enthalten ist (z.B. ersetzt durch eine andere Person). Wichtig ist, dass sich die beiden Datensammlungen in genau einem Datensatz unterscheiden.\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "    <b>Hinweis</b>\n",
    "    <br />\n",
    "    <br />\n",
    "    Bei dieser Betrachtungsweise geht es um die Grundidee von Differential Privacy, nicht um die Art und Weise wie Differential Privacy impementiert wird. In den nachfolgenden Notebooks wird noch gezeigt, wie die Implementierung von Differential Privacy aussehen kann.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed13bd3b",
   "metadata": {},
   "source": [
    "### Beispiel zur Grundidee von Differential Privacy\n",
    "\n",
    "Anhand eines Beispiels soll die oben beschriebene Grundidee von Differential Privacy verdeutlicht werden. Es sei die <a href=\"#tab-1\">Tabelle 1</a> mit Informationen zu den Augenfarben von Personen gegeben.\n",
    "\n",
    "| Vorname | Nachname | Augenfarbe |\n",
    "| :- | :- | :-| \n",
    "| Michael | Meier | blau |\n",
    "| Julia | Künzler | grün |\n",
    "| Karl | Lutz | blau |\n",
    "| Felix | Wagner | grau |\n",
    "| Maria | Ikonic | braun |\n",
    "| Jolanda | Meile | grün |\n",
    "| <span style=\"color:red\">Johannes</span> | <span style=\"color:red\">Sieber</span> | <span style=\"color:red\">blau</span> |\n",
    "| Angelina | Stucki | braun |\n",
    "| Mathias | Meister | braun |\n",
    "| Richard | Inauen | grün |\n",
    "| Daniela | Scheibner | braun |\n",
    "| Vanessa | Mayer | blau |\n",
    "\n",
    "<br>\n",
    "<a name=\"tab-1\"><center>Tabelle 1: Tabelle zu Augenfarben von Personen</center></a>\n",
    "\n",
    "Ziel des Angreifers: Die Augenfarbe von Johannes Sieber herausfinden. Der Angreifer verfügt über maximales Hintergrundwissen und kennt somit alle anderen Datensätze der Tabelle. \n",
    "\n",
    "Es wird nun eine Abfrage implementiert, welche es erlaubt die Anzahl Personen mit einer bestimmten Augenfarbe abzufragen.\n",
    "\n",
    "**Ohne Berücksichtigung der Differential Privacy** wäre dies eine einfache Aufgabe. Man würde einfach eine Funktion implementieren, welche die Personen mit der entsprechenden Augenfarbe zählt und ausgibt. Zum Beispiel wie folgt (pseudo SQL Statement):\n",
    "\n",
    "`COUNT(*)`  \n",
    "`FROM \"Tabelle 1\"`  \n",
    "`WHERE Augenfarbe = \"blau\"`  \n",
    "\n",
    "Ausgabe: `4`\n",
    "\n",
    "Der Angreifer weiss anhand dieser Ausgabe direkt, dass der unbekannte Datensatz offenbar die Augenfarbe \"blau\" haben muss. Denn der Angreifer kennt alle anderen 3 Datensätze mit der Augenfarbe \"blau\" ja bereits. Da die Ausgabe 4 ist, muss also der unbekannte Datensatz \"blau\" als Augenfarbe haben.\n",
    "\n",
    "Nun soll die Funktion so angepasst werden, dass diese die Eigenschaften der Differential Privacy erfüllt. Der Angreifer soll auf Basis der Ausgabe nicht mehr darauf schliessen können, welche Augenfarbe Johannes Sieber hat. Dafür muss ein bestimmtes Mass an Zufälligkeit der Ausgabe erreicht werden. Dies wird in vielen Fällen (aber nicht ausschliesslich) durch Hinzufügen von **Rauschen** erreicht. Das heisst also, dass nicht mehr die exakte Anzahl der Personen mit einer bestimmten Augenfarbe ausgegeben wird, sondern eine leicht veränderte. Das könnte dann wie folgt aussehen:\n",
    "\n",
    "\n",
    "`[COUNT(*)`  \n",
    "`FROM \"Tabelle 1\"`  \n",
    "`WHERE Augenfarbe = \"blau\"] + RANDOM(1,10)` (RANDOM soll eine Funktion sein, welche eine zufällige Ganzzahl zwischen 1-10 generiert)\n",
    "\n",
    "Ausgabe: `6` (RANDOM hat `2` als Zahl generiert)\n",
    "\n",
    "Aus Sicht des Angreifers sieht es nun so aus: Er zählt selbst 3 Personen mit der Augenfarbe \"blau\". Als Resultat der Abfrage hat er aber 6 erhalten, obwohl ja offensichtlich nur noch 1 Datensatz fehlt und somit das Ergebnis eigentlich maximal 4 hätte lauten dürfen. Da der Angreifer die zufällig addierte Zahl aber nicht kennt, weiss er nun nicht, ob Johannes Sieber in dieser Abfrage enthalten war oder nicht.\n",
    "\n",
    "Und genau das ist (stark vereinfacht) die Grundidee von Differential Privacy. Nur müssen für einen sinnvollen Einsatz von Differential Privacy natürlich bestimmte Regeln eingehalten und weitere Aspekte berücksichtigt werden. Beispielsweise muss die Grösse der Zufallszahl sinnvoll gewählt werden.\n",
    "\n",
    "Dazu ein Beispiel:\n",
    "\n",
    "`[COUNT(*)`  \n",
    "`FROM \"Tabelle 1\"`  \n",
    "`WHERE Augenfarbe = \"grau\"] + RANDOM(1,10)`\n",
    "\n",
    "Ausgabe: `10` (RANDOM hat `9` als Zahl generiert)\n",
    "\n",
    "In der Tabelle ist eigentlich nur eine Person mit der Augenfarbe \"grau\" enthalten. Als Resultat kam jedoch 10 zurück. Dies schützt zwar die Privacy, zerstört aber jeglichen Nutzen der Abfrage.\n",
    "\n",
    "Bei der Differential Privacy ist es von zentraler Bedeutung, dass korrekte Erkenntnisse über eine grosse Menge von Personen gewonnen werden können. Dies muss gegeben sein, ohne dass Erkenntnisse über einzelne Personen gewonnen werden können. Aus diesem Grund ist es essentiell, dass das Ergebnis nur so weit verändert wird, dass keine Erkenntnisse über Individuen gewonnen werden können, aber die Erkenntnisse über eine grosse Menge an Personen weiterhin relevant sind.\n",
    "\n",
    "In der Differential Privacy kommen bestimmte Ausgaben jeweils mit bestimmten Wahrscheinlichkeiten vor. Das heisst, dass die Ausgabe nicht mehr deterministisch ist, sondern probabilistisch. Die Anzahl der Personen mit blauen Augen wäre im obigen Beispiel also zu einer bestimmten Wahrscheinlichkeit `4`, zu einer anderen Wahrscheinlichkeit `5`, zu einer anderen Wahrscheinlichkeit `1`, usw. \n",
    "\n",
    "Daraus resultiert, dass das korrekte Resultat (in dem Beispiel mit den blauen Augen wäre dies `4`) nur zu einer bestimmten Wahrscheinlichkeit herauskommt. Die Wahrscheinlichkeiten spielen also eine zentrale Rolle bei der Differential Privacy. Deshalb gilt es nun die formale Definition der Differential Privacy anzuschauen."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69c6c614",
   "metadata": {},
   "source": [
    "## Die formale Definition von Differential Privacy\n",
    "\n",
    "Formal lässt sich das oben beschriebene Prinzip in folgende Definition übersetzen:\n",
    "\n",
    "<div class=\"alert alert-success\">\n",
    "<b>Definition: $\\varepsilon$-Differential Privacy</b>\n",
    "    <br />\n",
    "    <br /> \n",
    "Ein Mechanismus $M$ erfüllt dann die Eigenschaften von $\\varepsilon$-Differential Privacy, wenn für alle Datensammlungen $D_1$ und $D_2$, welche sich in genau einem Datensatz unterscheiden, für alle möglichen Ausgaben $A$ gilt:\n",
    "\n",
    "$$\\mathbb{P}[M(D_1) = A] \\leq e^{\\varepsilon} * \\mathbb{P}[M(D_2) = A]$$\n",
    "</div>\n",
    "\n",
    "Für eine bessere Verständlichkeit werden die einzelnen Teile des mathematischen Ausdrucks einzeln betrachtet:\n",
    "\n",
    "- $\\mathbb{P}[M(D_1) = A]$ beschreibt die Wahrscheinlichkeit, dass wenn der Mechanismus $M$ auf der Datensammlung $D_1$ ausgeführt wird, die Ausgabe $A$ resultiert. \n",
    "\n",
    "- $e^{\\varepsilon}$ ist die Eulersche Exponentialfunktion mit $\\varepsilon$ als Exponenten. Je kleiner das $\\varepsilon$, desto näher kommt die Exponentialfunktion an 1 (das $\\varepsilon$ muss per Definition $\\geq 0$ sein).\n",
    "\n",
    "- $\\mathbb{P}[M(D_2) = A]$ beschreibt die Wahrscheinlichkeit, dass wenn der Mechanismus $M$ auf der Datensammlung $D_2$ ausgeführt wird, die Ausgabe $A$ resultiert. Diese Wahrscheinlichkeit wird mit $e^{\\varepsilon}$ multipliziert. Das $\\varepsilon$ spielt eine essentielle Rolle bei der Differential Privacy und wird auch **Privacy-Budget** genannt.\n",
    "\n",
    "\n",
    "In Worte übersetzt, heisst das:\n",
    "\n",
    "\"Die Wahrscheinlichkeit, dass die Ausgabe A herauskommt, wenn die gesuchte Person enthalten ist, muss kleiner oder gleich sein, als die Wahrscheinlichkeit, dass die Ausgabe A herauskommt, wenn die gesuchte Person nicht enthalten ist. Die zweite Wahrscheinlichkeit wird mit dem Privacy-Budget multipliziert, was eine bewusste Lockerung der Definition ermöglicht. Durch das Privacy-Budget kann also festgelegt werden, wie stark man vom optimalen Schutz der Privacy abweichen möchte. Je grösser das Privacy-Budget, desto schlechter der Schutz.\"\n",
    "\n",
    "Die Definition ist symmetrisch. Das heisst, dass $D_1$ und $D_2$ vertauscht werden können, was formal wie folgt dargestellt werden kann:\n",
    "\n",
    "$$e^{-\\varepsilon} * \\mathbb{P}[M(D_2) = A] \\leq \\mathbb{P}[M(D_1) = A] \\leq e^{\\varepsilon} * \\mathbb{P}[M(D_2) = A]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50af1975",
   "metadata": {},
   "source": [
    "### Beispiel 1: Privacy-Budget von 0\n",
    "\n",
    "Es wird angenommen, dass die Ausgabe A mit einer Wahrscheinlichkeit von 0.034 basierend auf $D_1$ resultiert und mit Wahrscheinlichkeit von 0.028 basierend auf $D_2$. Diese Wahrscheinlichkeiten sind reine Annahmen und wurden nicht hergeleitet. Weiter wird ein Privacy-Budget von 0 verwendet.\n",
    "\n",
    "Gegeben sind somit:\n",
    "- $\\mathbb{P}[M(D_1) = A] = 0.034$\n",
    "- $\\mathbb{P}[M(D_2) = A] = 0.028$\n",
    "- $\\varepsilon = 0$\n",
    "\n",
    "Diese Werte werden nun in die Definition\n",
    "\n",
    "$$e^{-\\varepsilon} * \\mathbb{P}[M(D_2) = A] \\leq \\mathbb{P}[M(D_1) = A] \\leq e^{\\varepsilon} * \\mathbb{P}[M(D_2) = A]$$\n",
    "\n",
    "eingesetzt. Dies sieht dann wie folgt aus:\n",
    "\n",
    "\\begin{align*}\n",
    "& e^{-0} * 0.028 &\\leq 0.034 &\\leq e^{0} * 0.028 \\\\\n",
    "\\equiv & 1 * 0.028 &\\leq 0.034 &\\leq 1 * 0.028 \\\\\n",
    "\\equiv & 0.028 &\\leq 0.034 &\\leq 0.028\n",
    "\\end{align*}\n",
    "\n",
    "Der Term \"$0.028 \\leq 0.034 \\leq 0.028$\" ist offensichtlich FALSCH, weshalb die $0$-Differential Privacy NICHT erfüllt ist. \n",
    "\n",
    "Ein Privacy-Budget von 0 lässt, wie man sieht, keinen Spielraum für unterschiedliche Wahrscheinlichkeiten. Nur wenn die Wahrscheinlichkeiten $\\mathbb{P}[M(D_1) = A]$ und $\\mathbb{P}[M(D_2) = A]$ identisch sind, kann bei einem Privacy-Budget von 0 die Differential Privacy erfüllt werden.\n",
    "\n",
    "Dies macht zwar zum Schutz der Privacy Sinn, denn so resultieren alle Ausgaben A mit genau denselben Wahrscheinlichkeiten, egal ob die Person enthalten ($D_1$) oder nicht enthalten ($D_2$) war. In der Praxis macht ein Privacy-Budget von 0 jedoch keinen Sinn, da dadurch der Nutzen der Daten gänzlich verloren geht. Weshalb dies so ist, wird bei den praktischen Aufgaben in den nachfolgenden Notebooks noch deutlich gezeigt."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "725ed641",
   "metadata": {},
   "source": [
    "### Beispiel 2: Privacy-Budget von ln(2)\n",
    "\n",
    "Es werden dieselben Wahrscheinlichkeiten angenommen, wie im 1. Beispiel. Nun wird jedoch ein Privacy-Budget von ln(2) verwendet.\n",
    "\n",
    "Gegeben sind somit:\n",
    "- $\\mathbb{P}[M(D_1) = A] = 0.034$\n",
    "- $\\mathbb{P}[M(D_2) = A] = 0.028$\n",
    "- $\\varepsilon = ln(2)$\n",
    "\n",
    "Dies sieht wie folgt aus:\n",
    "\n",
    "\\begin{align*}\n",
    "& e^{-ln(2)} * 0.028 &\\leq 0.034 &\\leq e^{ln(2)} * 0.028 \\\\\n",
    "\\equiv & 0.5 * 0.028 &\\leq 0.034 &\\leq 2 * 0.028 \\\\\n",
    "\\equiv & 0.014 &\\leq 0.034 &\\leq 0.056\n",
    "\\end{align*}\n",
    "\n",
    "Der Term \"$0.014 \\leq 0.034 \\leq 0.056$\" ist WAHR und somit sind die Eigenschaften der $ln(2)$-Differential Privacy erfüllt. Dank dem Privacy Budget dürfen sich die Wahrscheinlichkeiten bis zu einem bestimmten Faktor unterscheiden, ohne dass die Differential Privacy verletzt wird."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5363dd7",
   "metadata": {},
   "source": [
    "### Beispiel 3: Privacy-Budget von ln(2) mit anderen Wahrscheinlichkeiten\n",
    "\n",
    "In diesem Beispiel werden andere Wahrscheinlichkeiten angenommen. Das Privacy-Budget bleibt weiterhin bei ln(2).\n",
    "\n",
    "Gegeben sind:\n",
    "- $\\mathbb{P}[M(D_1) = A] = 0.09$\n",
    "- $\\mathbb{P}[M(D_2) = A] = 0.001$\n",
    "- $\\varepsilon = ln(2)$\n",
    "\n",
    "\n",
    "\\begin{align*}\n",
    "& e^{-ln(2)} * 0.001 &\\leq 0.09 &\\leq e^{ln(2)} * 0.001 \\\\\n",
    "\\equiv & 0.5 * 0.001 &\\leq 0.09 &\\leq 2 * 0.001 \\\\\n",
    "\\equiv & 0.0005 &\\leq 0.09 &\\leq 0.002\n",
    "\\end{align*}\n",
    "\n",
    "Der Term \"$0.0005 \\leq 0.09 \\leq 0.002$\" ist FALSCH und somit sind die Eigenschaften der $ln(2)$-Differential Privacy NICHT erfüllt. Obwohl das Privacy-Budget gleich gewählt wurde wie bei Beispiel 2, sind nun die Eigenschaften der Differential Privacy nicht mehr erfüllt. Die Wahrscheinlichkeiten unterscheiden sich zu stark, um die Eigenschaften der Differential Privacy mit dem gegebenen Privacy-Budget zu erfüllen."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e52e21d1",
   "metadata": {},
   "source": [
    "### Beispiel 4: Privacy-Budget von 10 und extreme Wahrscheinlichkeiten\n",
    "\n",
    "In diesem Beispiel werden extrem unterschiedliche Wahrscheinlichkeiten angenommen. Zudem wird das Privacy-Budget mit einem Wert von 10 sehr hoch gewählt.\n",
    "\n",
    "Gegeben sind:\n",
    "- $\\mathbb{P}[M(D_1) = A] = 0.999$\n",
    "- $\\mathbb{P}[M(D_2) = A] = 0.005$\n",
    "- $\\varepsilon = 10$\n",
    "\n",
    "\n",
    "\\begin{align*}\n",
    "& e^{-10} * 0.005 &\\leq 0.999 &\\leq e^{10} * 0.005 \\\\\n",
    "\\cong & 0.00004539 * 0.005 &\\leq 0.999 &\\leq 22026 * 0.005 \\\\\n",
    "\\cong & 0.000000227 &\\leq 0.999 &\\leq 110\n",
    "\\end{align*}\n",
    "\n",
    "Der Term \"$0.000000227 \\leq 0.999 \\leq 110$\" ist WAHR und somit sind die Eigenschaften der $10$-Differential Privacy erfüllt. Die Interpretation der konkreten Zahlen lässt jedoch an der Sinnhaftigkeit der Wahrscheinlichkeiten zweifeln, denn:\n",
    "\n",
    "- Zu 99.9\\% kommt die Ausgabe $A$ heraus, wenn der Mechanismus $M$ über $D_1$ ausgeführt wird.\n",
    "- Wird der Mechanismus $M$ aber über $D_2$ ausgeführt, kommt nur zu 0.5\\% die Ausgabe $A$ heraus.\n",
    "- Obwohl nun also die Differential Privacy erfüllt ist, sind die Unterschiede der Ausgaben basierend auf $D_1$ und $D_2$ enorm. Dies, weil das Privacy-Budget zu hoch gewählt wurde und deshalb solch drastische Unterschiede in den Wahrscheinlichkeiten erlaubt.\n",
    "\n",
    "<div class=\"alert alert-warning\">\n",
    "<b>Wichtige Erkenntnis</b>\n",
    "    <br />\n",
    "    <br /> \n",
    "Nur weil die Eigenschaften von Differential Privacy erfüllt sind, heisst das nicht zwingend, dass auch ein guter Schutz der Privacy gegeben sein muss. Wie gut die Privacy geschützt wird, bestimmt das Privacy-Budget. Wird dieses Budget falsch (also zu hoch) festgelegt, kann dies zu unzureichendem Schutz der Privacy führen.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40f89165",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-danger\">\n",
    "    <h3>Übung: Wahrscheinlichkeiten und Privacy-Budget</h3>\n",
    "    <br />\n",
    "Die folgenden Werte sind gegeben:\n",
    "<ul>\n",
    "    <li>$\\mathbb{P}[M(D_1) = A] = 0.029$</li>\n",
    "    <li>$\\mathbb{P}[M(D_2) = A] = 0.036$</li>\n",
    "    <li>$\\varepsilon = 0.1$</li>\n",
    "</ul>\n",
    "\n",
    "Sind für die gegebenen Wahrscheinlichkeiten und für das gegebene Privacy-Budget die Eigenschaften der $0.1$-Differential Privacy erfüllt?\n",
    "<br />\n",
    "<br />\n",
    "<i>Hinweis: Eine Musterlösung ist im <a href=\"./7_Musterlösungen-der-Übungen.ipynb\">Notebook 7: Musterlösungen der Übungen</a> zu finden.</i>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f48e653f",
   "metadata": {},
   "source": [
    "## Quantifizierung des Angreiferwissens\n",
    "\n",
    "Die Bedeutung des Privacy-Budgets wird nachfolgend anhand eines Beispiels verdeutlicht. Hierbei wird gezeigt, wie das Wissen des Angreifers quantifiziert wird und welche Auswirkungen die Wahl des $\\varepsilon$-Werts hat.\n",
    "\n",
    "Es seien folgende Rahmenbedingungen gegeben:\n",
    "- Es sei ein Mechanismus $M$ gegeben, welcher die Eigenschaften von $\\varepsilon$-Differential Privacy erfüllt.\n",
    "- Dieser Mechanismus wird auf der Datensammlung $D$ ausgeführt, woraus die Ausgabe $M(D) = A$ resultiert.\n",
    "- Ein Angreifer versucht herauszufinden, ob die Zielperson in $D$ enthalten ist oder nicht. Für den Angreifer existieren somit zwei Szenarien: Die Zielperson ist in $D$ enthalten oder sie ist nicht in $D$ enthalten. Dies kann abstrahiert werden, indem zwischen zwei Datensammlungen unterschieden wird. $D_1$ für die Datensammlung, in welcher die Zielperson enthalten ist und $D_2$ für die Datensammlung, in welcher die Zielperson fehlt. Der Angreifer versucht anhand der Ausgabe $A$ herauszufinden, ob diese basierend auf $D_1$ oder $D_2$ zustande kam.\n",
    "- Es wird angenommen, dass der Angreifer maximales Hintergrundwissen hat und in der Datensammlung somit nur die Zielperson unbekannt ist. \n",
    "\n",
    "Der Angreifer hat einen initialen Verdacht, ob die Zielperson enthalten ist oder nicht. Dieser Verdacht kann zwischen $0$ (Angreifer weiss, dass Zielperson nicht enthalten ist) und $1$ (Angreifer weiss, dass Zielperson enthalten ist) liegen. Hat der Angreifer keinen Verdacht, ob die Zielperson enthalten ist oder nicht, liegt die Wahrscheinlichkeit bei $0.5$.\n",
    "\n",
    "Der Mechanismus $M$ wird ausgeführt und resultiert in der Ausgabe $A$. Nun stellt sich die Frage, um wie viel der Verdacht des Angreifers geändert hat, nachdem dieser die Ausgabe $A$ erhalten hat. Wird die Wahrscheinlichkeit des initialen Verdachts mit der Wahrscheinlichkeit des neuen Verdachts verglichen, erhält man einen genauen Wert für den Wissensgewinn des Angreifers. \n",
    "\n",
    "In <a href=\"#abb-4\">Abbildung 4</a> sind die unteren und oberen Grenzen des Wissensgewinns für unterschiedliche $\\varepsilon$-Werte grafisch dargestellt. Die weisse Linie von $(0.0|0.0)$ zu $(1.0|1.0)$ stellt den Fall dar, wenn der Angreifer kein zusätzliches Wissen gewonnen hat und der initiale Verdacht (\"initial suspicion\") dem neuen Verdacht (\"updated suspicion\") entspricht. Es ist ersichtlich, dass bei steigendem $\\varepsilon$-Wert der Wissensgewinn des Angreifers wächst. So würde ein initialer Verdacht von $0.1$ bei einem $\\varepsilon$-Wert von $5$ zu einem neuen Verdacht von $0.94$ führen. Der Angreifer wäre sich also nach einer einzigen Abfrage zu 94\\% sicher, dass die Zielperson in der Datensammlung enthalten ist, obwohl er sich zu Beginn nur zu 10\\% sicher war.\n",
    "<br>\n",
    "<br>\n",
    "<center>\n",
    "<img src=\"./src/dp-epsilon-values.png\" alt=\"Auswirkung Epsilon auf Wissensgewinn\">\n",
    "<br>\n",
    "<br>\n",
    "<a name=\"abb-4\">Abbildung 4: Auswirkung von $\\varepsilon$ auf den Wissensgewinn (Quelle: desfontain.es)</center></a>\n",
    "\n",
    "**Dies ist ein riesiger Vorteil der Differential Privacy!** Denn der potenzielle Wissenszuwachs eines Angreifers kann bestimmt werden. Bei der $k$-Anonymität z.B. sagt das $k$ nichts darüber aus, wie \"gut\" die Privacy ist. Ob nun $k = 4$ oder $k = 5$ gewählt wird, es lässt sich nicht direkt darauf schliessen, wie viel Wissen ein potenzieller Angreifer gewinnen kann oder wie effektiv der Schutz von Individuen innerhalb der Datensammlung gewährleistet ist."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d12c47b",
   "metadata": {},
   "source": [
    "### Das Privacy-Budget nochmals zusammengefasst\n",
    "\n",
    "Das $\\varepsilon$ erlaubt es den Privacy-Verlust genau zu quantifizieren und wird deshalb auch als \"Privacy-Budget\" bezeichnet. \n",
    "\n",
    "Das $\\varepsilon$ muss per Definition $\\geq 0$ sein. Je kleiner das $\\varepsilon$ wird, umso mehr nähert sich die Exponentialfunktion $e^{\\varepsilon}$ an $1$ an. Die Ähnlichkeit der Wahrscheinlichkeiten hängt somit stark von $\\varepsilon$ ab. Je kleiner das $\\varepsilon$ ist, desto ähnlicher sind die Ausgaben und somit wird auch der maximale Wissenszuwachs eines Angreifers kleiner.\n",
    "\n",
    "<div class=\"alert alert-warning\">\n",
    "<b>Wichtige Erkenntnis</b>\n",
    "    <br />\n",
    "    <br /> \n",
    "Je kleiner das $\\varepsilon$, umso höher der Schutz der Privacy.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "085a5996",
   "metadata": {},
   "source": [
    "## Komposition des Privacy-Budgets\n",
    "\n",
    "Ein weiterer grosser Vorteil von Differential Privacy ist, dass das Privacy-Budget auch für Kompositionen von mehreren Mechanismen bestimmt werden kann. Ganz im Kontrast zu beispielsweise der $k$-Anonymität. Bei der Kombination von zwei $k$-anonymen Datensammlungen, kann keine Aussage über den $k$-Wert der resultierten Datensammlung gemacht werden. Also wenn bspw. eine 3-anonyme und eine 5-anonyme Datensammlung vereint werden, kann daraus nicht direkt der neue $k$-Wert abgeleitet werden.\n",
    "\n",
    "Bei der Differential Privacy wird zwischen paralleler und sequentieller Komposition unterschieden. \n",
    "\n",
    "### Parallele Komposition\n",
    "\n",
    "Bei der **parallelen Komposition** (siehe auch <a href=\"#abb-5\">Abbildung 5</a>)  werden mehrere Mechanismen ausgeführt, wobei jeder Mechanismus über eine disjunkte Teilmenge der gesamten Datensammlung ausgeführt wird. Das heisst, dass ein Datensatz jeweils nur Einfluss auf das Ergebnis eines Mechanismus hat.\n",
    "<br>\n",
    "<br>\n",
    "<center>\n",
    "<img src=\"./src/Parallele-Komposition.png\" alt=\"Parallele Komposition\">\n",
    "<br>\n",
    "<br>\n",
    "<a name=\"abb-5\">Abbildung 5: Parallele Komposition</center></a>\n",
    "\n",
    "\n",
    "Die Komposition dieser Mechanismen ($M$) garantiert in diesem Fall das Privacy-Budget von $max(\\varepsilon_1, \\dots, \\varepsilon_m)$, was dem grössten $\\varepsilon$-Wert aller Mechanismen innerhalb der Komposition entspricht.\n",
    "\n",
    "#### Beispiel: Parallele Komposition\n",
    "\n",
    "Mechanismus 1: \"Zähle das Alter aller Männer zusammen\"  \n",
    "Mechanismus 2: \"Zähle das Alter aller Frauen zusammen\"\n",
    "\n",
    "\n",
    "Annahme: Jeder Datensatz kann nur entweder ein Mann oder eine Frau sein. \n",
    "\n",
    "Weiter nehmen wir folgende Privacy-Budgets für die beiden Mechanismen an:  \n",
    "$\\varepsilon$ Mechanismus 1 = 0.3  \n",
    "$\\varepsilon$ Mechanismus 2 = 0.6\n",
    "\n",
    "\n",
    "Die beiden Mechanismen werden über eine disjunkte Menge ausgeführt, sprich: das Alter eines Datensatzes kann entweder in Mechanismus 1 oder in Mechanismus 2 enthalten sein, da ein Datensatz nicht gleichzeitig eine Frau und ein Mann sein kann. Deshalb handelt es sich um eine parallele Komposition.\n",
    "\n",
    "Somit gilt: Privacy-Budget der Komposition = max(0.3, 0.6) = 0.6\n",
    "\n",
    "### Sequentielle Komposition\n",
    "\n",
    "Bei der **sequentiellen Komposition** (siehe auch <a href=\"#abb-6\">Abbildung 6</a>) werden die Mechanismen sequentiell und somit nacheinander auf derselben Datensammlung ausgeführt. \n",
    "<br>\n",
    "<br>\n",
    "<center>\n",
    "<img src=\"./src/Sequentielle-Komposition.png\" alt=\"Sequentielle Komposition\">\n",
    "<br>\n",
    "<br>\n",
    "<a name=\"abb-6\">Abbildung 6: Sequentielle Komposition</center></a>\n",
    "\n",
    "Die Komposition dieser Mechanismen ($M$) garantiert in diesem Fall das Privacy-Budget von $\\sum_{i=1}^m \\varepsilon$, was der Summe der $\\varepsilon$-Werte aller Mechanismen innerhalb der Komposition entspricht.\n",
    "\n",
    "#### Beispiel: Sequentielle Komposition\n",
    "\n",
    "Mechanismus 1: \"Zähle die Anzahl Personen mit blauen Augen\"  \n",
    "Mechanismus 2: \"Zähle die Anzahl Personen mit schwarzen Haaren\"\n",
    "\n",
    "Wir nehmen wiederum folgende Privacy-Budgets für die beiden Mechanismen an:  \n",
    "$\\varepsilon$ Mechanismus 1 = 0.3  \n",
    "$\\varepsilon$ Mechanismus 2 = 0.6\n",
    "\n",
    "Da die beiden Mechanismen über die gesamte Datensammlung ausgeführt werden, ist es eine sequentielle Komposition. Denn eine Person mit blauen Augen kann bspw. auch schwarze Haare haben.\n",
    "\n",
    "Somit gilt: Privacy-Budget der Komposition = sum(0.3, 0.6) = 0.9"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "991be9a9",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-danger\">\n",
    "    <h2>Übung: Komposition des Privacy-Budgets</h2>\n",
    "    <br />\n",
    "Es sei eine Datensammlung zu Straftaten gegeben. Über diese Datensammlung sollen nun zwei Mechanismen ausgeführt werden:  \n",
    "\n",
    "Mechanismus 1: \"Zähle die Anzahl Personen unter 18 Jahren\"  \n",
    "Mechanismus 2: \"Zähle die Anzahl Personen, bei welchen Drogenhandel als Straftat erfasst wurde\"\n",
    "    <br />\n",
    "    <br />\n",
    "    <b>Frage 1:</b> Welche Art von Komposition ist gegeben, wenn beide Mechanismen über der genannten Datensammlung ausgeführt werden?  \n",
    "    <b>Frage 2:</b> Für beide Mechanismen wird ein Privacy-Budget von 0.4 festgelegt. Wie gross ist das Privacy-Budget der Komposition beider Mechanismen?  \n",
    "<br />\n",
    "<br />\n",
    "<i>Hinweis: Eine Musterlösung ist im <a href=\"./7_Musterlösungen-der-Übungen.ipynb\">Notebook 7: Musterlösungen der Übungen</a> zu finden.</i>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9058785",
   "metadata": {},
   "source": [
    "## Kontrollfragen\n",
    "\n",
    "* Wie können die Eigenschaften von Differential Privacy für eine Datensammlung erreicht werden?\n",
    "* Wie heisst das Modell, bei welchem die Daten vor der Übermittlung an einen zentralen Dienstleister (Aggregator) verändert werden?\n",
    "* Ist die folgende Aussage korrekt? \"Je höher das Epsilon ($\\varepsilon$), desto höher der Schutz der Privacy.\"\n",
    "* Bei welcher Kompositions-Art werden die Privacy-Budgets addiert?\n",
    "\n",
    "_Hinweis: Eine Musterlösung der Kontrollfragen ist im nächsten Notebook zu finden._"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e0d288c",
   "metadata": {},
   "source": [
    "***Hier geht es zum nächsten Notebook dieser Serie: [Notebook 3: Der Laplace Mechanismus](./3_Laplace-Mechanismus.ipynb)***"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
